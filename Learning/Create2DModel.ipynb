{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%env TF_GPU_THREAD_MODE gpu_private\n",
    "import tensorflow as tf # env TF_GPU_THREAD_MODE=gpu_private somehow makes training much faster\n",
    "tf.config.experimental.set_memory_growth((tf.config.list_physical_devices('GPU'))[0], True)\n",
    "import tensorflow.keras as keras, tensorflow.keras.layers as layers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import import_ipynb\n",
    "import Dataset2d\n",
    "import MomentMetrics\n",
    "import custom_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, model, history, useYScale = False):\n",
    "    \"\"\"Evaluates a model\n",
    "     param dataset: The dataset, whose test attribut will be used for testing\n",
    "     param model: The model to be evaluated\n",
    "     param history: history to be plotted\n",
    "     param useYScale: whether to plot the yscale as symlog; Default: False \n",
    "    \"\"\"\n",
    "    metrics = model.evaluate(dataset.test, verbose=0)\n",
    "    print(\"Evaluated Metrics for the Test-Set:\")\n",
    "    for name, value in zip(model.metrics_names, (metrics if hasattr(metrics, '__iter__') else [metrics])):\n",
    "        print(f\"Metric: {name}: {value}\")\n",
    "    plt.plot(history.history['loss'],'b') \n",
    "    plt.plot(history.history['val_loss'],'r') \n",
    "    if useYScale: plt.yscale('symlog')\n",
    "    plt.show()\n",
    "    \n",
    "def build_model(num_out_dim = 4, input_shape = (8), num_out_vectors= 1, denseLayers = [], convLayers = []):\n",
    "    \"\"\"Builds the model. \n",
    "     param num_out_dim: number of dimensions defined by the labels, aka in 2D Sim available\n",
    "     param input_shape: shape of model input\n",
    "     param num_out_vectors: number of output vectors. Output shape will therefore be num_out_vectors x num_out_dim\n",
    "     param denseLayers: list of number of neurons per dense layer \n",
    "     param convLayers: list of convLayers. Each convLayer consists of a list [num_filters, kernel_diameter, pool_diameter].\n",
    "         The model is added a Conv2D(filters=num_filters, kernel_size=(kernel_diameter,kernel_diameter)).BatchNorm.MaxPool2D((pool_diameter,pool_diameter))\n",
    "    \"\"\"\n",
    "    model = keras.Sequential(name=\"Testmodel_2D\")\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    for i,c in enumerate(convLayers):\n",
    "        nfilters, kernel_d, pool_d = c\n",
    "        model.add(layers.Conv2D(filters=nfilters, kernel_size=(kernel_d,kernel_d), activation='relu',name=f\"Conv_{i}\"))\n",
    "        model.add(layers.BatchNormalization(name=f\"BN_{i}\"))\n",
    "        model.add(layers.MaxPool2D(pool_size=(pool_d, pool_d),name=f\"Pool_{i}\"))\n",
    "    if(convLayers): model.add(layers.Flatten(name=\"Flatten_to_Dense\")) # only flatten if convLayers is not empty\n",
    "    for i,d in enumerate(denseLayers): \n",
    "        model.add(layers.Dense(d, activation='relu', name=f\"Dense_{i}\"))\n",
    "    model.add(layers.Dense(num_out_vectors*num_out_dim, name=\"Final_Paths\"))\n",
    "    model.add(layers.Reshape((num_out_vectors,num_out_dim), name=\"Shape_to_paths\"))\n",
    "    model.add(custom_layers.PathNormalisation_layer(name=\"Normalise_paths\"))\n",
    "    model.add(custom_layers.Covariance_layer(name=\"Cov\"))\n",
    "    return model\n",
    "\n",
    "def buildAndTrainModel(dataset, num_out_vectors, loss, epochs, metrics = [], printSummary=True, callbacks=[], denseLayers=[], convLayers = [], input_shape = (8)):\n",
    "    \"\"\"Builds and trains a model\n",
    "     param dataset: Dataset used for training\n",
    "     param num_out_vectors: number of dimensions defined by the labels, aka in 2D Sim available\n",
    "     param loss: Loss to be applied during training\n",
    "     param epochs: Target number of epochs used during training\n",
    "     param metrics: List of metrics used in training. Default: []\n",
    "     param printSummary: Whether to print the model summary. Default: True\n",
    "     param callbacks: Callback functions forwarded to tf. Default: [], \n",
    "     param denseLayers: list of number of neurons per dense layer in the model. Default:[]\n",
    "     param convLayers: list of convLayers. Each convLayer consists of a list [num_filters, kernel_diameter, pool_diameter]. Default: [] \n",
    "     param input_shape: shape of model input. Default: (8)\n",
    "    \"\"\"\n",
    "    model = build_model(num_out_vectors = num_out_vectors,input_shape = input_shape, denseLayers = denseLayers, convLayers=convLayers) # define model\n",
    "    if(printSummary): model.summary()\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "    history = model.fit(dataset.train, epochs=epochs, batch_size=None, shuffle=False, callbacks=callbacks, \n",
    "                        validation_data=ds.test)\n",
    "    evaluate(dataset, model, history)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, prepare and display the dataset\n",
    "path=\"../Datasets/Dataset_square_boxes/\"\n",
    "ds = Dataset2d.Dataset2d(path, {\"train_test_ratio\":20, \"step_size\":10, \"batch_size\":64,\\\n",
    "        'render_images':True, 'render_poles':False})\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "model, history = buildAndTrainModel(ds, epochs=2, \n",
    "                                    input_shape=(600,600,3), num_out_vectors=12,\n",
    "                                    convLayers=[[3,3,6],[4,3,6],[5,3,4]],\n",
    "                                    denseLayers=[10,10,20],\n",
    "                                    callbacks = [\n",
    "                                        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2) \n",
    "                                        #,tf.keras.callbacks.TensorBoard()\n",
    "                                        ],\n",
    "                                    printSummary=True, \n",
    "                                    loss = MomentMetrics.momentLoss_onlyExponent_onSigma, metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print examples of network output\n",
    "## Green Arrows: label user input\n",
    "## Orange Arrows: First proposed DoF\n",
    "## Yellow Arrows: Second proposed DoF\n",
    "def custom_func(feat, label, model, loss):\n",
    "    return\n",
    "    \n",
    "Dataset2d.plotDatapointBatch(ds.test, numPlots=20, figsize=(5,5), model=model,useEigenvectors=False, useSigma=True, \n",
    "    loss=MomentMetrics.momentLoss_onlyExponent_onSigma, custom_func=custom_func, printTexts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store calculated model in default location\n",
    "storagePath = \"./model/\"\n",
    "if not os.path.exists(storagePath): os.makedirs(storagePath)\n",
    "Dataset2d.storeModel(model, storagePath)\n",
    "with open(storagePath+\"about.txt\", 'w') as file: # About File:\n",
    "    datasetName = path.split(\"/\")[-2] if path[-1] == '/' else path.split(\"/\")[-1]\n",
    "    about = f\"Dataset: {datasetName}\\nNumber of epochs: {history.params['epochs']}\\n\"+\\\n",
    "            f\"\\n\\n{ds}\\n\\n\"\n",
    "    file.write(about)\n",
    "    model.summary(print_fn=lambda x: file.write(x + '\\n'))\n",
    "    about2 = f\"Metrics per epoch in training: {history.history}\\n\\n\"\n",
    "    file.write(about2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model from default location to namedModels\n",
    "val = input(\"Name to store Model under (Warning, must contain _poles to work with poles): \")\n",
    "if(val):\n",
    "    import shutil, pathlib\n",
    "    the_path = pathlib.Path(f\"../JS_Simulation/namedModels/{val}\")\n",
    "    the_path.mkdir(exist_ok=True)\n",
    "    for file in [\"about.txt\", \"model.json\", \"model_weights.h5\"]:\n",
    "        shutil.copy(f\"{storagePath}/{file}\", f\"{the_path}/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
