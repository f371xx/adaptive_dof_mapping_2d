{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics using the main Moments (or Covariance Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function \n",
    "Label $y$: $$y = \\text{y_true} \\in \\mathbb{R}^{n\\times1} \\quad \\text{, with} \\quad n = 7 = \\texttt{NUM_OUT_DIM}$$\n",
    "\n",
    "Network-Output $o$: $$o = \\text{pred_vects} \\in \\mathbb{R}^{m\\times 7} \\quad \\text{, with} \\quad m = \\texttt{NUM_OUT_CHANNELS}$$\n",
    "\n",
    "$$o_{ij} = \\left(t_i\\right)_j \\quad\\text{ or }o = \\begin{pmatrix}\n",
    "t_0\\\\ t_1\\\\\\vdots\\\\ t_m\\end{pmatrix}, t_i \\in \\mathbb{R}^{1\\times7}$$\n",
    "\n",
    "Reinterpretation of the Network Output (different proposed paths $t_i$) as \"Covariance Matrix\" aka Matrix of Main-Moments $\\Sigma$: $$ \\Sigma = \\epsilon I_n + \\frac{1}{m}\\sum\\limits_{i}^m t_i^T\\cdot t_i      \\quad \\text{, with}\\quad I_n\\text{ as Identity-Matrix}$$\n",
    "\n",
    "Loss $l$: $$\\begin{align}\n",
    "l &= -\\ln p\\left(y|x;\\Sigma\\right)\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Multidimensional Gaussian Distribution: $$\n",
    "\\begin{align}\n",
    "p(y) &= (2\\pi)^{-\\frac{n}{2}} \\cdot \\left|\\Sigma\\right|^{-\\frac{1}{2}} \\cdot \\exp\\left(-\\frac{1}{2}\\left(y-\\mu\\right)^T\\cdot\\Sigma^{-1}\\cdot \\left(y-\\mu\\right)\\right) \\\\\n",
    "\\Rightarrow l &=  -\\left( \\underbrace{-\\left(\\frac{n}{2}\\right)\\cdot\\ln\\left(2\\pi\\right)  -\\frac{1}{2}\\ln\\left|\\Sigma\\right|}_{\\text{previously scaling factor}}  \\underbrace{-\\frac{1}{2}y^T \\Sigma^{-1} y}_{\\text{previously exponent}}         \\right)\\\\\n",
    "&= \\frac{1}{2}\\left(\\underbrace{n \\cdot\\ln\\left(2\\pi\\right)}_{\\text{constant offset}}  + \\ln\\left|\\Sigma\\right| + y^T \\Sigma^{-1} y \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "With the assumption: $\\mu =0$, so that the network does not have a preference to drive ie. forwards without input.\n",
    "\n",
    "### Formating in Code\n",
    "`label (y_true)` is a (flat) row Tensor of shape: $\\begin{pmatrix}t_0,t_1,\\dots,t_n\\end{pmatrix}$, **not** $\\begin{pmatrix}\n",
    "t_0\\\\t_1\\\\\\vdots\\\\t_n\\end{pmatrix}$ : ``'tf.Tensor([...], shape=(7,), dtype=float32)'``\n",
    "\n",
    "`network-output (pred_vects)` is a tensorflow Tensor of shape (batch, m, 7), therefore also a stack of (flat) row Tensors\n",
    "\n",
    "Adjusted loss (without constant offset $\\left(n\\cdot\\ln\\left(2\\pi\\right)\\right)$ and factors $\\left(\\frac{1}{2}\\right)$) used in code:\n",
    "\n",
    "$$ \\hat{l} =   \\ln\\left|\\Sigma\\right| + y^T \\Sigma^{-1} y          $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.set_memory_growth((tf.config.list_physical_devices('GPU'))[0], True)\n",
    "from math import pi\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def getSigma(pred_vects):\n",
    "    \"\"\"Calculates Sigma (Covariance Matrix) based on the network output pred_vects\n",
    "    Format pred_vects: (batch_size x m x n) , with n=number of dimensions\n",
    "    Calculation: Sum of outer Products of every set of dimensions with itself and divide by number of sets to normalise . (and add epsilon)\n",
    "    Method: for each batch: pred_vects^T    @     pred_vects\n",
    "                            {shape n x m} matmul {shape n x m}\n",
    "    Returns sigma of shape (batch_size x n x n)\n",
    "    \n",
    "    Adds an epsilon of tf.keras.backend.epsilon() (*Identity Matrix) to ensure Invertabillity\n",
    "    \"\"\"\n",
    "    sigma = tf.matmul(pred_vects, pred_vects, transpose_a=True) \n",
    "    sigma = sigma / pred_vects.shape[-2] # Divide by m to normalise sigma invariant of number of input vectors\n",
    "    # Add epsilon to sigma to avoid zeros\n",
    "    sigma = tf.add(sigma, tf.eye(sigma.shape[-1], batch_shape=sigma.shape[:0]) * tf.keras.backend.epsilon())\n",
    "    return sigma\n",
    "\n",
    "@tf.function\n",
    "def momentProb(y_true, pred_vects):\n",
    "    \"\"\"Returns the probability of the label y_true given the output of the network pred_vects\n",
    "    \n",
    "    Format y_true: (batch_size x n) , with n=numer of dimensions\n",
    "    Format pred_vects: (batch_size x m x n) , with n=number of dimensions\n",
    "    \n",
    "    Returns probability per batch as (batch_size)\n",
    "    \n",
    "    Probability = (2 pi)^(-n/2) * 1/sqrt(det(Sigma)) * exp(-1/2 y^T * Sigma^(-1) * y)\n",
    "    \"\"\"\n",
    "    sigma = getSigma(pred_vects)\n",
    "    \n",
    "    scalingFactor = (2*pi)**(-(sigma.shape[-1])/2) \n",
    "    det_part = tf.math.reciprocal_no_nan(tf.sqrt(tf.linalg.det(sigma)))\n",
    "    inv_sigma = tf.linalg.inv(sigma)\n",
    "    y = tf.expand_dims(y_true, axis=-1) # necessary to enable matmul to work with batch x n x 1, undone with squeeze in next line\n",
    "    exponent = -1/2 *  tf.squeeze(tf.matmul(tf.matmul(y, inv_sigma, transpose_a=True), y))\n",
    "    return scalingFactor * det_part * tf.exp(exponent)\n",
    "\n",
    "@tf.function\n",
    "def momentLoss(y_true, pred_vects):\n",
    "    \"\"\"Calculates the loss of the label value y_true based on the Gaussian Distribution defined by the network output (pred_vects)\n",
    "    Uses the probability based on Multidimensional Gaussian Distribution, but removes the standard offset of n*ln(2pi) and the factor 1/2 \n",
    "\n",
    "    Format y_true: (batch_size x n) , with n=numer of dimensions\n",
    "    Format pred_vects: (batch_size x m x n) , with n=number of dimensions\n",
    "    \n",
    "    Returns loss per batch as (batch_size x 1)\n",
    "    \"\"\"\n",
    "    sigma = getSigma(pred_vects)\n",
    "    det_part = tf.linalg.logdet(sigma) # = log(det(sigma))\n",
    "    #det_part = tf.math.log(tf.linalg.det(sigma)+tf.keras.backend.epsilon()) # = log(det(sigma))\n",
    "    inv_sigma = tf.linalg.inv(sigma)\n",
    "    y = tf.expand_dims(y_true, axis=-1) # necessary to enable matmul to work with batch x n x 1, undone with squeeze in next line\n",
    "    exponent = tf.squeeze(tf.matmul(tf.matmul(y, inv_sigma, transpose_a=True), y)) \n",
    "    return det_part + exponent\n",
    "\n",
    "@tf.function\n",
    "def momentLoss_onlyExponent(y_true, pred_vects):\n",
    "    \"\"\"Equal to Momentloss, but without the determinant part\"\"\"\n",
    "    sigma = getSigma(pred_vects)\n",
    "    inv_sigma = tf.linalg.inv(sigma)\n",
    "    y = tf.expand_dims(y_true, axis=-1) # necessary to enable matmul to work with batch x n x 1, undone with squeeze in next line\n",
    "    exponent = tf.squeeze(tf.matmul(tf.matmul(y, inv_sigma, transpose_a=True), y)) \n",
    "    return exponent\n",
    "\n",
    "@tf.function\n",
    "def momentLoss_onlyExponent_onSigma(y_true, sigma):\n",
    "    \"\"\"Equal to Momentloss, but without the determinant part and based on sigma\"\"\"\n",
    "    inv_sigma = tf.linalg.inv(sigma)\n",
    "    y = tf.expand_dims(y_true, axis=-1) # necessary to enable matmul to work with batch x n x 1, undone with squeeze in next line\n",
    "    exponent = tf.squeeze(tf.matmul(tf.matmul(y, inv_sigma, transpose_a=True), y)) \n",
    "    return exponent\n",
    "\n",
    "@tf.function\n",
    "def momentLoss_onSigma(y_true, sigma):\n",
    "    \"\"\"Equal to momentLoss, but with given sigma\"\"\"\n",
    "    sigma = tf.add(sigma, tf.eye(sigma.shape[-1], batch_shape=sigma.shape[:0]) * tf.keras.backend.epsilon())\n",
    "    det_part = tf.linalg.logdet(sigma) # = log(det(sigma))\n",
    "    #det_part = tf.math.log(tf.linalg.det(sigma)+tf.keras.backend.epsilon()) # = log(det(sigma))\n",
    "    inv_sigma = tf.linalg.inv(sigma)\n",
    "    y = tf.expand_dims(y_true, axis=-1) # necessary to enable matmul to work with batch x n x 1, undone with squeeze in next line\n",
    "    exponent = tf.squeeze(tf.matmul(tf.matmul(y, inv_sigma, transpose_a=True), y)) \n",
    "    return det_part + exponent\n",
    "    \n",
    "@tf.function\n",
    "def momentLoss2(y_true, pred_vects):\n",
    "    \"\"\"Slower variant of momentLoss, but with mathematical correcter result (= -ln(prob) )\n",
    "    \n",
    "    Calculates the loss of the label value y_true based on the Gaussian Distribution defined by the network output (pred_vects)\n",
    "    Uses the probability based on Multidimensional Gaussian Distribution, without removing the standard offset of n*ln(2pi) and the factor 1/2 \n",
    "    \n",
    "    Format y_true: (batch_size x n) , with n=numer of dimensions\n",
    "    Format pred_vects: (batch_size x m x n) , with n=number of dimensions\n",
    "    \n",
    "    Returns loss per batch as (batch_size x 1)\n",
    "    \"\"\"\n",
    "    sigma = getSigma(pred_vects)\n",
    "    const_offset = sigma.shape[-1] * tf.math.log(2 * pi)\n",
    "    det_part = tf.math.log(tf.linalg.det(sigma))\n",
    "    inv_sigma = tf.linalg.inv(sigma)\n",
    "    y = tf.expand_dims(y_true, axis=-1) # necessary to enable matmul to work with batch x n x 1, undone with squeeze in next line\n",
    "    exponent = tf.squeeze(tf.matmul(tf.matmul(y, inv_sigma, transpose_a=True), y)) \n",
    "    return 1/2 * (const_offset + det_part + exponent)\n",
    "\n",
    "@tf.function\n",
    "def getEigen(pred_vects):\n",
    "    \"\"\"Returns the Eigenvalues and Eigenvectors defined by the network output pred_vects\n",
    "    Format: [eigenvalues, eigenvectors], in the order of eigenvalues by tf.linalg.eigh\n",
    "    \n",
    "    Warning: eigenvectors is an square matrix. Each Vector is in a column. Therefore\n",
    "     you CANNOT iterate through eigenvectors with i in eigenVectors. Use Transpose!\n",
    "    \"\"\"\n",
    "    sigma = getSigma(pred_vects)\n",
    "    return tf.linalg.eigh(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print and Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "def plotOutput(pred_vects, figsize=(20,10), dimension_names=[\"X\",\"Y\",\"Z\",\"Rot_X\",\"Rot_Y\",\"Rot_Z\",\"Finger\"],\n",
    "              cmap =\"gray\", showPure=True, showSigma=True, showEigen=True, storeImages=None, plotProbTrue=None):\n",
    "    \"\"\"Plots the network output pred_vects of a single prediction as images\n",
    "    param figsize: 2-Dim Shape defining the figure size of the Eigenvector Plot\n",
    "    param dimension_names: list of dimension names. Must be of same length as pred_vects[-1]\n",
    "    param cmap: cmap parameter to be passed on to matpotlib\n",
    "    param showPure: weather to handle the pure output\n",
    "    param showSigma: weather to handle the Sigma Covariance matrix\n",
    "    param showEigen: weather to handle the Eigenvectors\n",
    "    param storeImages: Path to store the handled images to\n",
    "    param plotProbTrue: true value to plot. Has to be set to a vector of same size as eigenvektor\n",
    "    \n",
    "    \"\"\"\n",
    "    assert tf.rank(pred_vects) == 2, \"plotOutput requires output of single prediction, not batch.\"\n",
    "    sigma = getSigma(pred_vects)\n",
    "    ndims = sigma.shape[-1]\n",
    "    if showPure:\n",
    "        # Plot network output\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.suptitle(\"Output Vectors\")\n",
    "        for i,v in enumerate(pred_vects):\n",
    "            plt.subplot(1,len(pred_vects),1+i)\n",
    "            plt.imshow(abs(np.expand_dims(v, axis=1)), cmap=cmap)\n",
    "            plt.xticks([])\n",
    "            plt.yticks(range(ndims), dimension_names)\n",
    "            for j, vi in enumerate(v):\n",
    "                plt.text(0,j,f\"{vi:.1f}\", horizontalalignment='center', verticalalignment='center')\n",
    "        plt.tight_layout()\n",
    "        if storeImages:\n",
    "            plt.savefig(os.path.join(storeImages,\"output.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    if showSigma:\n",
    "        # Plot Sigma\n",
    "        plt.figure()\n",
    "        plt.title(\"Output Sigma $\\Sigma$\")\n",
    "        plt.imshow(abs(sigma.numpy()), cmap=cmap)\n",
    "        plt.yticks(range(ndims), dimension_names)\n",
    "        plt.xticks(range(ndims), dimension_names)\n",
    "        plt.tight_layout()\n",
    "        if storeImages:\n",
    "            plt.savefig(os.path.join(storeImages,\"covariance.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    if showEigen:\n",
    "        # Plot Eigenvectors and values in descending order\n",
    "        vals, vecs = tf.linalg.eigh(sigma) \n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.suptitle(\"Eigenvalues and -vectors\")\n",
    "        for i, (val, vec) in enumerate(zip(vals, tf.transpose(vecs))):\n",
    "            plt.subplot(1,ndims,ndims-i)\n",
    "            plt.title(f\"$\\lambda=${val.numpy():.2f}\")\n",
    "            plt.imshow(abs(np.expand_dims(vec.numpy(), axis=1)), cmap=cmap)\n",
    "            plt.xticks([])\n",
    "            plt.yticks(range(ndims), dimension_names)\n",
    "            plt.xlabel(f\"loss: {momentLoss(vec, pred_vects):.2f}\")\n",
    "            for j, v in enumerate(vec):\n",
    "                plt.text(0,j,f\"{v:.1f}\", horizontalalignment='center', verticalalignment='center')\n",
    "        plt.tight_layout()\n",
    "        if storeImages:\n",
    "            plt.savefig(os.path.join(storeImages,\"eigenvectors.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    if not plotProbTrue == None:\n",
    "        plt.plot(plotProbTrue)\n",
    "        plt.imshow(abs(np.expand_dims(plotProbTrue, axis=1)), cmap=cmap)\n",
    "        plt.xticks([])\n",
    "        plt.yticks(range(ndims), dimension_names)\n",
    "        plt.xlabel(f\"loss: {momentLoss(plotProbTrue, pred_vects):.2f}\")\n",
    "        for j, v in enumerate(plotProbTrue):\n",
    "            plt.text(0,j,f\"{v:.1f}\", horizontalalignment='center', verticalalignment='center')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras prerequisites:\n",
    "- requests (assumes during compiling) labels and network-output to have the same rank\n",
    "- network output has shape `( batch_size, ... )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Test metrics\n",
    "    out = [[\n",
    "           [4,0,0],\n",
    "           [0,2,0],\n",
    "           [0,1,0],\n",
    "           [0,1,0],\n",
    "          ],[\n",
    "           [5,0,0],\n",
    "           [0,3,0],\n",
    "           [0,1,0],\n",
    "           [0,1,0],\n",
    "          ]]\n",
    "    out = tf.convert_to_tensor(out,dtype=tf.dtypes.float32)\n",
    "    \n",
    "    print(f\"Network shape: {out.shape}: (batch-size x m_vectors x n_dims)\")\n",
    "    print(f\"Output:\")\n",
    "    for o in out:\n",
    "        plotOutput(o, figsize=(4,2), dimension_names=[\"X\",\"Y\",\"Z\"])\n",
    "        break\n",
    "    def printlabel(label):\n",
    "        label= tf.convert_to_tensor(label,dtype=tf.dtypes.float32)\n",
    "        label = tf.math.l2_normalize(label, axis=1)\n",
    "        loss, prob, loss2 = momentLoss(label, out), momentProb(label,out) , momentLoss2(label, out)\n",
    "        print(f\"label: {label}\\n    Loss    : {loss}\\n    Prob    : {prob}\\n    Loss2   : {loss2}\\n    ln(prob): {-tf.math.log(prob)}\")\n",
    "    print(\"--- TRIVIAL CASES: ---\")\n",
    "    printlabel([[1,0,0],[0,1,0]])\n",
    "    printlabel([[0,1,0],[0,1,0]])\n",
    "    print(\"\\n-- Should cause Errors (be less probable) -- \")\n",
    "    printlabel([[0,0,1],[0,0,1]])\n",
    "    printlabel([[0,1,1],[0,1,1]])\n",
    "    printlabel([[1,1,1],[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
